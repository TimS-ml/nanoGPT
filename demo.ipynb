{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "from typing import Optional, Tuple, Union, List\n",
    "from jaxtyping import Float, Bool\n",
    "\n",
    "from boring_utils.utils import get_device, cprint, tprint\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):\n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, max_seq_len: int = 1024):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % num_heads == 0, f\"n_embed {embedding_dim} must be divisible by num_heads {num_heads}\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = embedding_dim // num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(embedding_dim, 3 * embedding_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "\n",
    "        self.register_buffer(\n",
    "                \"mask\", \n",
    "                torch.tril(torch.ones(max_seq_len, max_seq_len))\n",
    "                    .view(1, 1, max_seq_len, max_seq_len))  # extend dims to 4\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: Float[Tensor, \"batch seq_len embedding_dim\"]\n",
    "        ) -> Float[Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        batch, seq_len, embedding_dim = x.shape\n",
    "\n",
    "        # [\"batch, seq_len, embedding_dim\"] -> [\"batch, seq_len, (3 * embedding_dim)\"]\n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = qkv.split(self.embedding_dim, dim=-1)  # split at the last dim\n",
    "\n",
    "        # embedding_dim -> num_heads * head_dim\n",
    "        # put seq_len and the head_dim together\n",
    "        q, k, v = map(lambda t: rearrange(t, 'batch seq_len (num_heads head_dim) -> batch num_heads seq_len head_dim', num_heads = self.num_heads), (q, k, v))\n",
    "\n",
    "        norm_factor = 1.0 / np.sqrt(embedding_dim)\n",
    "        attn = (q @ k.transpose(-2, -1)) * norm_factor\n",
    "        attn = attn.masked_fill(self.mask[:, :, :seq_len, :seq_len] == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        # attn: [batch, num_heads, seq_len, seq_len]\n",
    "        # v:    [batch, num_heads, seq_len, head_dim]\n",
    "        # y:    [batch, num_heads, seq_len, head_dim]\n",
    "        y = attn @ v\n",
    "        y = rearrange(y, 'batch num_heads seq_len head_dim -> batch seq_len (num_heads head_dim)')\n",
    "        return self.out_proj(y)  # [batch, seq_len, embedding_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        hidden_dim = embedding_dim * 4\n",
    "        self.fc_1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.activation = nn.GELU(approximate='tanh')\n",
    "        self.fc_2 = nn.Linear(hidden_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x: Float[Tensor, \"batch seq_len embedding_dim\"]) -> Float[Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        # no skip connection here\n",
    "        return self.fc_2(self.activation(self.fc_1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(embedding_dim))  # scaling\n",
    "        self.beta = nn.Parameter(torch.zeros(embedding_dim))  # offset \n",
    "        self.eps = eps  # small value to prevent division by zero\n",
    "    \n",
    "    def forward(self, x: Float[torch.Tensor, \"batch seq_len embedding_dim\"]) -> Float[torch.Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        mean = x.mean(dim=-1, keepdim=True)  # [batch, seq_len, 1]\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)  # [batch, seq_len, 1]\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)  # [batch, seq_len, embedding_dim]\n",
    "        return self.gamma * x_norm + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, max_seq_len: int = 1024):\n",
    "        super().__init__()\n",
    "        # self.ln_1 = nn.LayerNorm(embedding_dim)  # norm on the last dim\n",
    "        # self.ln_2 = nn.LayerNorm(embedding_dim)\n",
    "        self.ln_1 = LayerNorm(embedding_dim)  # norm on the last dim\n",
    "        self.ln_2 = LayerNorm(embedding_dim)\n",
    "        self.attn = CausalSelfAttention(num_heads, embedding_dim, max_seq_len)\n",
    "        self.ffn = FFN(embedding_dim)\n",
    "    \n",
    "    def forward(self, x: Float[Tensor, \"batch seq_len embedding_dim\"]) -> Float[Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        # skip connection, pre-layer norm\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.ffn(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_size: int = 50257,\n",
    "            num_heads: int = 12, \n",
    "            embedding_dim: int = 768, \n",
    "            max_seq_len: int = 1024, \n",
    "            num_layers: int = 12,\n",
    "            dropout_rate: float = 0.0\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(vocab_size, embedding_dim),\n",
    "            wpe = nn.Embedding(max_seq_len, embedding_dim),\n",
    "            drop = nn.Dropout(dropout_rate),\n",
    "            h = nn.ModuleList([TransformerBlock(num_heads, embedding_dim, max_seq_len) for _ in range(num_layers)]),\n",
    "            ln_f = nn.LayerNorm(embedding_dim, bias=False)\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: Float[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        x = self.transformer.wte(x) + self.transformer.wpe(x)\n",
    "        x = self.transformer.drop(x)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        return self.lm_head(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
