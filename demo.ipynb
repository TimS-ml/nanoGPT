{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import Tensor\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "from typing import Optional, Tuple, Union, List\n",
    "from jaxtyping import Float, Bool\n",
    "\n",
    "from boring_utils.utils import get_device, cprint, tprint\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):\n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, max_seq_len: int = 1024, bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % num_heads == 0, f\"n_embed {embedding_dim} must be divisible by num_heads {num_heads}\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = embedding_dim // num_heads\n",
    "\n",
    "        self.c_attn = nn.Linear(embedding_dim, 3 * embedding_dim, bias=bias)  # qkv projection\n",
    "        self.c_proj = nn.Linear(embedding_dim, embedding_dim, bias=bias)  # output projection\n",
    "\n",
    "        self.register_buffer(\n",
    "                \"mask\", \n",
    "                torch.tril(torch.ones(max_seq_len, max_seq_len))\n",
    "                    .view(1, 1, max_seq_len, max_seq_len))  # extend dims to 4\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: Float[Tensor, \"batch seq_len embedding_dim\"]\n",
    "        ) -> Float[Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        batch, seq_len, embedding_dim = x.shape\n",
    "\n",
    "        # [\"batch, seq_len, embedding_dim\"] -> [\"batch, seq_len, (3 * embedding_dim)\"]\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.embedding_dim, dim=-1)  # split at the last dim\n",
    "\n",
    "        # embedding_dim -> num_heads * head_dim\n",
    "        # put seq_len and the head_dim together\n",
    "        q, k, v = map(lambda t: rearrange(t, 'batch seq_len (num_heads head_dim) -> batch num_heads seq_len head_dim', num_heads = self.num_heads), (q, k, v))\n",
    "\n",
    "        norm_factor = 1.0 / np.sqrt(k.size(-1))  # k.size(-1) is the head_dim\n",
    "        attn = (q @ k.transpose(-2, -1)) * norm_factor\n",
    "        attn = attn.masked_fill(self.mask[:, :, :seq_len, :seq_len] == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        # attn: [batch, num_heads, seq_len, seq_len]\n",
    "        # v:    [batch, num_heads, seq_len, head_dim]\n",
    "        # y:    [batch, num_heads, seq_len, head_dim]\n",
    "        y = attn @ v\n",
    "        y = rearrange(y, 'batch num_heads seq_len head_dim -> batch seq_len (num_heads head_dim)')\n",
    "        return self.c_proj(y)  # [batch, seq_len, embedding_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualSelfAttention_alternative(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, max_seq_len: int = 1024, bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % num_heads == 0, f\"n_embed {embedding_dim} must be divisible by num_heads {num_heads}\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = embedding_dim // num_heads\n",
    "\n",
    "        # self.qkv_proj = nn.Linear(embedding_dim, 3 * embedding_dim, bias=False)\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'key': nn.Linear(embedding_dim, self.head_size, bias=bias),\n",
    "                'query': nn.Linear(embedding_dim, self.head_size, bias=bias), \n",
    "                'value': nn.Linear(embedding_dim, self.head_size, bias=bias)\n",
    "            }) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.c_proj = nn.Linear(embedding_dim, embedding_dim, bias=bias)  # output projection\n",
    "\n",
    "        self.register_buffer(\n",
    "                \"mask\", \n",
    "                torch.tril(torch.ones(max_seq_len, max_seq_len))\n",
    "                    .view(1, 1, max_seq_len, max_seq_len))  # extend dims to 4\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: Float[Tensor, \"batch seq_len embedding_dim\"]\n",
    "        ) -> Float[Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        batch, seq_len, embedding_dim = x.shape\n",
    "\n",
    "        # cat([batch, seq_len, head_dim] x num_heads) -> [batch, seq_len, num_heads * head_dim]\n",
    "        q = torch.cat([h['query'](x) for h in self.heads], dim=-1)\n",
    "        k = torch.cat([h['key'](x) for h in self.heads], dim=-1)\n",
    "        v = torch.cat([h['value'](x) for h in self.heads], dim=-1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'batch seq_len (num_heads head_dim) -> batch num_heads seq_len head_dim', num_heads = self.num_heads), (q, k, v))\n",
    "\n",
    "        norm_factor = 1.0 / np.sqrt(k.size(-1))  # k.size(-1) is the head_dim\n",
    "        attn = (q @ k.transpose(-2, -1)) * norm_factor\n",
    "        attn = attn.masked_fill(self.mask[:, :, :seq_len, :seq_len] == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        # attn: [batch, num_heads, seq_len, seq_len]\n",
    "        # v:    [batch, num_heads, seq_len, head_dim]\n",
    "        # y:    [batch, num_heads, seq_len, head_dim]\n",
    "        y = attn @ v\n",
    "        y = rearrange(y, 'batch num_heads seq_len head_dim -> batch seq_len (num_heads head_dim)')\n",
    "        return self.c_proj(y)  # [batch, seq_len, embedding_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GELU\n",
    "$$ \\text{GELU}(x) = x \\cdot \\Phi(x) $$\n",
    "\n",
    "Where $ \\Phi(x) $ is the CDF. The approximation involves the term $ 0.5 \\cdot (1 + \\tanh(\\sqrt{2/\\pi}(x +\n",
    "0.044715x^3))) $, and the cubic term with 0.044715 helps correct the approximation, particularly in the tails of\n",
    "the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    # Gaussian Error Linear Units\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        hidden_dim = embedding_dim * 4\n",
    "        self.c_fc = nn.Linear(embedding_dim, hidden_dim, bias=bias)\n",
    "        # self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.gelu = GELU()\n",
    "        self.c_proj = nn.Linear(hidden_dim, embedding_dim, bias=bias)\n",
    "\n",
    "    def forward(self, x: Float[Tensor, \"batch seq_len embedding_dim\"]) -> Float[Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        # no skip connection here\n",
    "        return self.c_proj(self.gelu(self.c_fc(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(embedding_dim))  # scaling (gamma)\n",
    "        self.bias = nn.Parameter(torch.zeros(embedding_dim))  # offset (beta)\n",
    "        self.eps = eps  # small value to prevent division by zero\n",
    "    \n",
    "    def forward(self, x: Float[torch.Tensor, \"batch seq_len embedding_dim\"]) -> Float[torch.Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        mean = x.mean(dim=-1, keepdim=True)  # [batch, seq_len, 1]\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)  # [batch, seq_len, 1]\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)  # [batch, seq_len, embedding_dim]\n",
    "        return self.weight * x_norm + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Transformer Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, max_seq_len: int = 1024, bias: bool = True):\n",
    "        super().__init__()\n",
    "        # self.ln_1 = nn.LayerNorm(embedding_dim, bias=bias)  # norm on the last dim\n",
    "        # self.ln_2 = nn.LayerNorm(embedding_dim, bias=bias)\n",
    "        self.ln_1 = LayerNorm(embedding_dim)  # norm on the last dim\n",
    "        self.ln_2 = LayerNorm(embedding_dim)\n",
    "        self.attn = CasualSelfAttention(num_heads, embedding_dim, max_seq_len, bias=bias)\n",
    "        self.mlp = FFN(embedding_dim, bias=bias)\n",
    "    \n",
    "    def forward(self, x: Float[Tensor, \"batch seq_len embedding_dim\"]) -> Float[Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        # skip connection, pre-layer norm\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT and Load Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/miniforge3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf:    ['transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias']\n",
      "mine:  ['transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x TransformerBlock(\n",
       "        (ln_1): LayerNorm()\n",
       "        (ln_2): LayerNorm()\n",
       "        (attn): CasualSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): FFN(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_size: int = 50257,\n",
    "            num_heads: int = 12, \n",
    "            embedding_dim: int = 768, \n",
    "            max_seq_len: int = 1024, \n",
    "            num_layers: int = 12,\n",
    "            dropout_rate: float = 0.0,\n",
    "            bias: bool = True\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(vocab_size, embedding_dim),\n",
    "            wpe = nn.Embedding(max_seq_len, embedding_dim),\n",
    "            drop = nn.Dropout(dropout_rate),\n",
    "            h = nn.ModuleList([TransformerBlock(num_heads, embedding_dim, max_seq_len, bias=bias) for _ in range(num_layers)]),\n",
    "            # ln_f = nn.LayerNorm(embedding_dim, bias=bias)\n",
    "            ln_f = LayerNorm(embedding_dim)\n",
    "        ))\n",
    "        # TODO: why bias=False?\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x: Float[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        batch, seq_len = x.shape\n",
    "        assert seq_len <= self.max_seq_len, f\"input length {seq_len} is longer than max seq length {self.max_seq_len}\"\n",
    "\n",
    "        pos = torch.arange(0, seq_len, device=x.device)\n",
    "        pos_emb = self.transformer.wpe(pos)  # [seq_len, embedding_dim]\n",
    "        tok_emb = self.transformer.wte(x)  # [batch, seq_len, embedding_dim]\n",
    "        x = tok_emb + pos_emb  # [batch, seq_len, embedding_dim]\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "        return self.lm_head(x)  # [batch, seq_len, vocab_size]\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        '''https://youtu.be/l8pRSuU81PU?t=1830\n",
    "        '''\n",
    "        assert model_type in {'gpt2'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        model = GPT()\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.mask')]  # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]  # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.mask')]  # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        print('hf:   ', [k for k in sd_keys_hf if \"h.0\" in k])\n",
    "        print('mine: ', [k for k in sd_keys if \"h.0\" in k])\n",
    "\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding using Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(enc, question, num_attempt=3, max_length=100):\n",
    "    # tokenizer encode\n",
    "    tokens = enc.encode(question)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    tokens = tokens.unsqueeze(0).repeat(num_attempt, 1)\n",
    "    x = tokens.to(device)\n",
    "\n",
    "    while x.size(1) < max_length:\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)  # (B, T, vocab_size)\n",
    "\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        # turn to zero for all indices below the top-k\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        # [Multinomial distribution - Wikipedia](https://en.wikipedia.org/wiki/Multinomial_distribution)\n",
    "        ix = torch.multinomial(topk_probs, 1)  # (B, 1)\n",
    "\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix)  # (B, 1)\n",
    "\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "\n",
    "    # print the generated text\n",
    "    for i in range(num_attempt):\n",
    "        tprint(f'{i + 1}th Attempt:')\n",
    "        tokens = x[i, :max_length].tolist()\n",
    "\n",
    "        # tokenizer decode\n",
    "        decoded = enc.decode(tokens)\n",
    "        print(f\"> {decoded}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== generate_text -> 1th Attempt: ====================\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: There's a lot to do in order to become part of the gang in order to become a leader, and to be part of the gang. You make a lot of decisions that give you a chance to make a decision based on everything that you're doing.\n",
      "\n",
      "Teenager: You think this is too difficult?\n",
      "\n",
      "Assistant: No. Actually, I think it is too difficult, so you can't give the right\n",
      "\n",
      "\n",
      "==================== generate_text -> 2th Attempt: ====================\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: The police have been looking for people who work in law enforcement, but who have never got to that stage. They like you to ask some questions, but you probably aren't even close to the one that is coming up soon. There are a few you can ask, but first of all, tell us what kind of position you would like to have, and who are your friends.\n",
      "\n",
      "Assistant: That's a nice question\n",
      "\n",
      "\n",
      "==================== generate_text -> 3th Attempt: ====================\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: I'm on a job. We're in business. We have an opportunity.\n",
      "\n",
      "The girl asks if I can go with him to a location to make the night safer. I do not give her my real name. I am a former employee of the organization which is the biggest mafia in this country. When they were killing all members of the gang, they created this \"gangland.\" As they were killing their members,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "QUESTION = \"How do I become a gang leader?\"\n",
    "INPUT_TEXT = f\"Human: {QUESTION}\\n\\nAssistant:\"\n",
    "generate_text(enc, INPUT_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== generate_text -> 1th Attempt: ====================\n",
      "> How do I become a gang leader?\n",
      "\n",
      "No, but you gotta know some secrets. (He was asking about his family)\n",
      "\n",
      "Is that why you're not married?\n",
      "\n",
      "Yes. (He didn't know your past. He wasn't sure his future).\n",
      "\n",
      "Do you have any other secrets?\n",
      "\n",
      "Yes, I guess.\n",
      "\n",
      "Doesn't it make you sick to your stomach?\n",
      "\n",
      "I know.\n",
      "\n",
      "If what is in your past were to\n",
      "\n",
      "\n",
      "==================== generate_text -> 2th Attempt: ====================\n",
      "> How do I become a gang leader? I get along a lot better with the boys compared to what I know from my family.\"\n",
      "\n",
      "While the pair's friendship was at first awkward to the boys, they quickly grew closer and became friends when he enrolled at UCLA. He found inspiration when his teammates decided to run on the Los Angeles Chargers. After a year of play, the boys came to the conclusion that the team needed a better pass rush. In early April, Chargers backup cornerback Ryan Ramc\n",
      "\n",
      "\n",
      "==================== generate_text -> 3th Attempt: ====================\n",
      "> How do I become a gang leader?\n",
      "\n",
      "Well first off, if you can prove that being a gang member is better you've done something wrong and you've also done something wrong for people with disabilities or the disabled.\n",
      "\n",
      "You need a clear, concise and concise way of thinking of your case.\n",
      "\n",
      "By the end of the day you should have a clear objective definition of what you are.\n",
      "\n",
      "You will get the benefit of training, experience, connections and connections from people who\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_text(enc, QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE (Byte Pair Encoding)\n",
    "\n",
    "```python\n",
    "r\"\"\"'s|'t|'re|'ve|'m|'ll|'d  匹配一些常见的英语缩略形式,如 's, 't, 're, 've, 'm, 'll, 'd \n",
    "\\p{L}+                       匹配任何Unicode字母字符的序列(如英语单词)\n",
    "\\p{N}+                       匹配任何Unicode数字字符的序列(如123,3.14等) \n",
    "[^\\s\\p{L}\\p{N}]+             匹配任何不是空白、字母或数字的字符序列(如标点符号、特殊字符等)\n",
    "\\s+(?!\\S)                    匹配连续空白符(但后面不能紧跟非空白字符)\n",
    "\\s+                          匹配任何其他连续空白符\n",
    " ?                           匹配一个可选的空格\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In utf-8:\n",
    "- 0-31 是控制字符,比如 \\x00 是空字符, \\x01 是开头, \\x09 是制表符等\n",
    "- 32-127 是基本拉丁字母、数字和部分标点符号\n",
    "- 128-255 是扩展ASCII码,包括了一些重音字母和特殊字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m<module> -> bytes_to_unicode()[ord(b'\\x21')]:\u001b[0m\n",
      "'!'\n",
      "\u001b[93m<module> -> bytes_to_unicode()[33]:\u001b[0m\n",
      "'!'\n"
     ]
    }
   ],
   "source": [
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Every possible byte (really an integer 0..255) gets mapped by OpenAI to a unicode\n",
    "    character that represents it visually.\n",
    "    \"\"\"\n",
    "    # the 188 integers that render fine in their original form and need no shifting\n",
    "    printable_bytes = \\\n",
    "        list(range(ord(\"!\"), ord(\"~\")+1)) + \\\n",
    "        list(range(ord(\"¡\"), ord(\"¬\")+1)) + \\\n",
    "        list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "\n",
    "    unicode_chars = printable_bytes[:] \n",
    "    shift_count = 0\n",
    "    for byte in range(256):\n",
    "        if byte not in printable_bytes:\n",
    "            # if this byte is \"ugly\" then map it to the next available \"nice\" character\n",
    "            printable_bytes.append(byte)\n",
    "            unicode_chars.append(256 + shift_count)\n",
    "            shift_count += 1\n",
    "            \n",
    "    unicode_chars = [chr(n) for n in unicode_chars]\n",
    "    byte_to_char_map = dict(zip(printable_bytes, unicode_chars))\n",
    "    return byte_to_char_map\n",
    "\n",
    "\n",
    "# NOTE: Don't be fooled by the printed output, the dict should be {b'\\x21': '!', b'\\x22': '\"', ...} instead of {33: '!', 34: '\"', ...}\n",
    "cprint(bytes_to_unicode()[ord(b'\\x21')])\n",
    "cprint(bytes_to_unicode()[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m<module> -> bytes_to_unicode():\u001b[0m\n",
      "{33: '!', 34: '\"', 35: '#', 36: '$', 37: '%', 38: '&', 39: \"'\", 40: '(', 41: ')', 42: '*', 43: '+', 44: ',', 45: '-', 46: '.', 47: '/', 48: '0', 49: '1', 50: '2', 51: '3', 52: '4', 53: '5', 54: '6', 55: '7', 56: '8', 57: '9', 58: ':', 59: ';', 60: '<', 61: '=', 62: '>', 63: '?', 64: '@', 65: 'A', 66: 'B', 67: 'C', 68: 'D', 69: 'E', 70: 'F', 71: 'G', 72: 'H', 73: 'I', 74: 'J', 75: 'K', 76: 'L', 77: 'M', 78: 'N', 79: 'O', 80: 'P', 81: 'Q', 82: 'R', 83: 'S', 84: 'T', 85: 'U', 86: 'V', 87: 'W', 88: 'X', 89: 'Y', 90: 'Z', 91: '[', 92: '\\\\', 93: ']', 94: '^', 95: '_', 96: '`', 97: 'a', 98: 'b', 99: 'c', 100: 'd', 101: 'e', 102: 'f', 103: 'g', 104: 'h', 105: 'i', 106: 'j', 107: 'k', 108: 'l', 109: 'm', 110: 'n', 111: 'o', 112: 'p', 113: 'q', 114: 'r', 115: 's', 116: 't', 117: 'u', 118: 'v', 119: 'w', 120: 'x', 121: 'y', 122: 'z', 123: '{', 124: '|', 125: '}', 126: '~', 161: '¡', 162: '¢', 163: '£', 164: '¤', 165: '¥', 166: '¦', 167: '§', 168: '¨', 169: '©', 170: 'ª', 171: '«', 172: '¬', 174: '®', 175: '¯', 176: '°', 177: '±', 178: '²', 179: '³', 180: '´', 181: 'µ', 182: '¶', 183: '·', 184: '¸', 185: '¹', 186: 'º', 187: '»', 188: '¼', 189: '½', 190: '¾', 191: '¿', 192: 'À', 193: 'Á', 194: 'Â', 195: 'Ã', 196: 'Ä', 197: 'Å', 198: 'Æ', 199: 'Ç', 200: 'È', 201: 'É', 202: 'Ê', 203: 'Ë', 204: 'Ì', 205: 'Í', 206: 'Î', 207: 'Ï', 208: 'Ð', 209: 'Ñ', 210: 'Ò', 211: 'Ó', 212: 'Ô', 213: 'Õ', 214: 'Ö', 215: '×', 216: 'Ø', 217: 'Ù', 218: 'Ú', 219: 'Û', 220: 'Ü', 221: 'Ý', 222: 'Þ', 223: 'ß', 224: 'à', 225: 'á', 226: 'â', 227: 'ã', 228: 'ä', 229: 'å', 230: 'æ', 231: 'ç', 232: 'è', 233: 'é', 234: 'ê', 235: 'ë', 236: 'ì', 237: 'í', 238: 'î', 239: 'ï', 240: 'ð', 241: 'ñ', 242: 'ò', 243: 'ó', 244: 'ô', 245: 'õ', 246: 'ö', 247: '÷', 248: 'ø', 249: 'ù', 250: 'ú', 251: 'û', 252: 'ü', 253: 'ý', 254: 'þ', 255: 'ÿ', 0: 'Ā', 1: 'ā', 2: 'Ă', 3: 'ă', 4: 'Ą', 5: 'ą', 6: 'Ć', 7: 'ć', 8: 'Ĉ', 9: 'ĉ', 10: 'Ċ', 11: 'ċ', 12: 'Č', 13: 'č', 14: 'Ď', 15: 'ď', 16: 'Đ', 17: 'đ', 18: 'Ē', 19: 'ē', 20: 'Ĕ', 21: 'ĕ', 22: 'Ė', 23: 'ė', 24: 'Ę', 25: 'ę', 26: 'Ě', 27: 'ě', 28: 'Ĝ', 29: 'ĝ', 30: 'Ğ', 31: 'ğ', 32: 'Ġ', 127: 'ġ', 128: 'Ģ', 129: 'ģ', 130: 'Ĥ', 131: 'ĥ', 132: 'Ħ', 133: 'ħ', 134: 'Ĩ', 135: 'ĩ', 136: 'Ī', 137: 'ī', 138: 'Ĭ', 139: 'ĭ', 140: 'Į', 141: 'į', 142: 'İ', 143: 'ı', 144: 'Ĳ', 145: 'ĳ', 146: 'Ĵ', 147: 'ĵ', 148: 'Ķ', 149: 'ķ', 150: 'ĸ', 151: 'Ĺ', 152: 'ĺ', 153: 'Ļ', 154: 'ļ', 155: 'Ľ', 156: 'ľ', 157: 'Ŀ', 158: 'ŀ', 159: 'Ł', 160: 'ł', 173: 'Ń'}\n"
     ]
    }
   ],
   "source": [
    "cprint(bytes_to_unicode(), use_pprint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    \"\"\"\n",
    "    https://tiktokenizer.vercel.app/?model=gpt2\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder: dict = None, bpe_merges: dict = None):\n",
    "        # encoder: map bytes to unicode characters\n",
    "        # decoder: inverse of encoder\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k,v in self.byte_encoder.items()}\n",
    "\n",
    "        # encoder: bpe token to index, json dict\n",
    "        # {... \"clud\": 758, \"tern\": 759, \"\\u0120know\": 760 ...}\n",
    "        # decoder: index to bpe token\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "\n",
    "        # bpe merge list that defines the bpe \"tree\"\n",
    "        # {... Ġre claimed, Ġinteresting ly, × ©, rom y, J M, ĠEnhance ment, ...}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "\n",
    "        self.gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "        self.cache = {}\n",
    "\n",
    "        # ids:     [239, 188, 181, 239, 189, ]\n",
    "        # ids[1:]: [188, 181, 239, 189, ]\n",
    "        # pairs: [(239, 188), (188, 181), (181, 239), (239, 189), ]\n",
    "        self.get_pairs = lambda word: set(zip(word, word[1:]))\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        tokens = [self.decoder[i] for i in ids]\n",
    "        tokens_flat = ''.join(tokens)\n",
    "\n",
    "        # recovering 'Ġ' -> ' '\n",
    "        tokens_bytes = bytearray([self.byte_decoder[c] for c in tokens_flat])\n",
    "        text = tokens_bytes.decode('utf-8', errors='replace')\n",
    "        return text\n",
    "\n",
    "    def bpe_merge(self, token: str) -> str:\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "\n",
    "        word = tuple(token)\n",
    "        pairs = self.get_pairs(word)\n",
    "        if not pairs: return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "\n",
    "            if bigram not in self.bpe_ranks: break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "\n",
    "                # find the next occurence of first in the sequence of current words\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                # if this occurence is also followed by second, then merge them into one\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "\n",
    "            # all occurences of (first, second) have been merged to first_second\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = self.get_pairs(word)\n",
    "\n",
    "        # concat all words into a string, and use ' ' as the separator. Note that\n",
    "        # by now all characters have been byte encoded, guaranteeing that ' ' is\n",
    "        # not used in the actual data and is a 'special' delimiter character\n",
    "        word = ' '.join(word)\n",
    "\n",
    "        # cache the result and return\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        bpe_idx = []\n",
    "        # pre-tokenize the input text into a list of string tokens, this is the minimum unit of tokenization\n",
    "        # input: \"Hello've world123!!!?    \"\n",
    "        # output: ['Hello', \"'ve\", ' world', '123', '!!!', '?', '    ']\n",
    "        tokens = re.findall(self.gpt2pat, text)\n",
    "\n",
    "        for token in tokens:\n",
    "            # char to bytes\n",
    "            token_bytes = token.encode('utf-8')\n",
    "\n",
    "            # apply the openai byte encoder to the token, ' word' -> 'Ġword'\n",
    "            token_translated = ''.join(self.byte_encoder[b] for b in token_bytes)\n",
    "\n",
    "            # perform all the applicable bpe merges according to self.bpe_ranks\n",
    "            # 'interestingly' -> 'interest' + 'ingly'\n",
    "            token_merged = self.bpe_merge(token_translated).split(' ')\n",
    "\n",
    "            # translate all bpe tokens to integers\n",
    "            # 'interest' + 'ingly' -> [9446, 4420]\n",
    "            token_ix = [self.encoder[bpe_token] for bpe_token in token_merged]\n",
    "\n",
    "            # extend our running list of all output integers\n",
    "            bpe_idx.extend(token_ix)\n",
    "        return bpe_idx\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls):\n",
    "        data_dir = './data/gpt2_tokenizer/'\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "        # load encoder.json that has the raw mappings from token -> bpe index\n",
    "        encoder_path = os.path.join(data_dir, 'encoder.json')\n",
    "        if not os.path.isfile(encoder_path):\n",
    "            encoder_remote_url = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json'\n",
    "            response = requests.get(encoder_remote_url)\n",
    "            open(encoder_path, \"wb\").write(response.content)\n",
    "        with open(encoder_path, 'r') as f:\n",
    "            encoder = json.load(f)\n",
    "        assert len(encoder) == 50257  # 256 individual byte tokens, 50,000 merged tokens, and 1 special <|endoftext|> token\n",
    "\n",
    "        # load vocab.bpe that contains the bpe merges, i.e. the bpe tree structure\n",
    "        vocab_path = os.path.join(data_dir, 'vocab.bpe')\n",
    "        if not os.path.isfile(vocab_path):\n",
    "            vocab_remote_url = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe'\n",
    "            response = requests.get(vocab_remote_url)\n",
    "            open(vocab_path, \"wb\").write(response.content)\n",
    "        with open(vocab_path, 'r', encoding=\"utf-8\") as f:\n",
    "            bpe_data = f.read()\n",
    "        bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "        assert len(bpe_merges) == 50000  # 50,000 merged tokens\n",
    "\n",
    "        # construct the Encoder object and return\n",
    "        enc = Encoder(encoder, bpe_merges)\n",
    "        return enc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Tiktoken with Our Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== generate_text -> 1th Attempt: ====================\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: The next time you're in a gang with others, don't start shooting any more. Kill everyone. Do it now.\n",
      "\n",
      "Busty: I know, I know.\n",
      "\n",
      "Assassin: He can make you.\n",
      "\n",
      "Assistant: There must be some way you could help him.\n",
      "\n",
      "[The next time the squad enters the prison, we hear an alarm and the guards at the hospital are searching,\n",
      "\n",
      "\n",
      "==================== generate_text -> 2th Attempt: ====================\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: As the Gang Leader. By the way, I'm not doing this for the money. I'm doing this to make a living and hopefully give my kids the chance to succeed.\n",
      "\n",
      "Trial: How much time will it take you to achieve that?\n",
      "\n",
      "Assistant: I would need 100 days to prepare me. I have to show them where I am on a daily basis, because if I have to work 60 hours\n",
      "\n",
      "\n",
      "==================== generate_text -> 3th Attempt: ====================\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Giant: You can take all of these skills in two parts.\n",
      "\n",
      "Assistant:\n",
      "\n",
      "I have three skills.\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Me, do you like your job.\n",
      "\n",
      "Assistant:\n",
      "\n",
      "We're friends. We need you right now.\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Why are you here?\n",
      "\n",
      "Assistant:\n",
      "\n",
      "You could do anything if you wanted.\n",
      "\n",
      "Assistant:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enc = Encoder.from_pretrained()\n",
    "generate_text(enc, INPUT_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    # Pythonic way to iterate consecutive elements\n",
    "    # ids:     [239, 188, 181, 239, 189, ]\n",
    "    # ids[1:]: [188, 181, 239, 189, ]\n",
    "    # pairs: [(239, 188), (188, 181), (181, 239), (239, 189), ]\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def single_merge(ids, pair, idx):\n",
    "    # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "    # single_merge([5, 6, 6, 7, 9, 1], (6, 7), 99) -> [5, 6, 99, 9, 1]\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if we are not at the very last position AND the pair matches, replace it\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# top_pair = max(stats, key=stats.get)\n",
    "# tokens2 = merge(tokens, top_pair, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
