{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import Tensor\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "from typing import Optional, Tuple, Union, List\n",
    "from jaxtyping import Float, Bool\n",
    "\n",
    "from boring_utils.utils import get_device, cprint, tprint\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):\n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, max_seq_len: int = 1024, bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % num_heads == 0, f\"n_embed {embedding_dim} must be divisible by num_heads {num_heads}\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = embedding_dim // num_heads\n",
    "\n",
    "        self.c_attn = nn.Linear(embedding_dim, 3 * embedding_dim, bias=bias)  # qkv projection\n",
    "        self.c_proj = nn.Linear(embedding_dim, embedding_dim, bias=bias)  # output projection\n",
    "\n",
    "        self.register_buffer(\n",
    "                \"mask\", \n",
    "                torch.tril(torch.ones(max_seq_len, max_seq_len))\n",
    "                    .view(1, 1, max_seq_len, max_seq_len))  # extend dims to 4\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: Float[Tensor, \"batch seq_len embedding_dim\"]\n",
    "        ) -> Float[Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        batch, seq_len, embedding_dim = x.shape\n",
    "\n",
    "        # [\"batch, seq_len, embedding_dim\"] -> [\"batch, seq_len, (3 * embedding_dim)\"]\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.embedding_dim, dim=-1)  # split at the last dim\n",
    "\n",
    "        # embedding_dim -> num_heads * head_dim\n",
    "        # put seq_len and the head_dim together\n",
    "        q, k, v = map(lambda t: rearrange(t, 'batch seq_len (num_heads head_dim) -> batch num_heads seq_len head_dim', num_heads = self.num_heads), (q, k, v))\n",
    "\n",
    "        norm_factor = 1.0 / np.sqrt(k.size(-1))  # k.size(-1) is the head_dim\n",
    "        attn = (q @ k.transpose(-2, -1)) * norm_factor\n",
    "        attn = attn.masked_fill(self.mask[:, :, :seq_len, :seq_len] == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        # attn: [batch, num_heads, seq_len, seq_len]\n",
    "        # v:    [batch, num_heads, seq_len, head_dim]\n",
    "        # y:    [batch, num_heads, seq_len, head_dim]\n",
    "        y = attn @ v\n",
    "        y = rearrange(y, 'batch num_heads seq_len head_dim -> batch seq_len (num_heads head_dim)')\n",
    "        return self.c_proj(y)  # [batch, seq_len, embedding_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualSelfAttention_alternative(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, max_seq_len: int = 1024, bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % num_heads == 0, f\"n_embed {embedding_dim} must be divisible by num_heads {num_heads}\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = embedding_dim // num_heads\n",
    "\n",
    "        # self.qkv_proj = nn.Linear(embedding_dim, 3 * embedding_dim, bias=False)\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'key': nn.Linear(embedding_dim, self.head_size, bias=bias),\n",
    "                'query': nn.Linear(embedding_dim, self.head_size, bias=bias), \n",
    "                'value': nn.Linear(embedding_dim, self.head_size, bias=bias)\n",
    "            }) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.c_proj = nn.Linear(embedding_dim, embedding_dim, bias=bias)  # output projection\n",
    "\n",
    "        self.register_buffer(\n",
    "                \"mask\", \n",
    "                torch.tril(torch.ones(max_seq_len, max_seq_len))\n",
    "                    .view(1, 1, max_seq_len, max_seq_len))  # extend dims to 4\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: Float[Tensor, \"batch seq_len embedding_dim\"]\n",
    "        ) -> Float[Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        batch, seq_len, embedding_dim = x.shape\n",
    "\n",
    "        # cat([batch, seq_len, head_dim] x num_heads) -> [batch, seq_len, num_heads * head_dim]\n",
    "        q = torch.cat([h['query'](x) for h in self.heads], dim=-1)\n",
    "        k = torch.cat([h['key'](x) for h in self.heads], dim=-1)\n",
    "        v = torch.cat([h['value'](x) for h in self.heads], dim=-1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'batch seq_len (num_heads head_dim) -> batch num_heads seq_len head_dim', num_heads = self.num_heads), (q, k, v))\n",
    "\n",
    "        norm_factor = 1.0 / np.sqrt(k.size(-1))  # k.size(-1) is the head_dim\n",
    "        attn = (q @ k.transpose(-2, -1)) * norm_factor\n",
    "        attn = attn.masked_fill(self.mask[:, :, :seq_len, :seq_len] == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        # attn: [batch, num_heads, seq_len, seq_len]\n",
    "        # v:    [batch, num_heads, seq_len, head_dim]\n",
    "        # y:    [batch, num_heads, seq_len, head_dim]\n",
    "        y = attn @ v\n",
    "        y = rearrange(y, 'batch num_heads seq_len head_dim -> batch seq_len (num_heads head_dim)')\n",
    "        return self.c_proj(y)  # [batch, seq_len, embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        hidden_dim = embedding_dim * 4\n",
    "        self.c_fc = nn.Linear(embedding_dim, hidden_dim, bias=bias)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(hidden_dim, embedding_dim, bias=bias)\n",
    "\n",
    "    def forward(self, x: Float[Tensor, \"batch seq_len embedding_dim\"]) -> Float[Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        # no skip connection here\n",
    "        return self.c_proj(self.gelu(self.c_fc(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(embedding_dim))  # scaling (gamma)\n",
    "        self.bias = nn.Parameter(torch.zeros(embedding_dim))  # offset (beta)\n",
    "        self.eps = eps  # small value to prevent division by zero\n",
    "    \n",
    "    def forward(self, x: Float[torch.Tensor, \"batch seq_len embedding_dim\"]) -> Float[torch.Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        mean = x.mean(dim=-1, keepdim=True)  # [batch, seq_len, 1]\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)  # [batch, seq_len, 1]\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)  # [batch, seq_len, embedding_dim]\n",
    "        return self.weight * x_norm + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, max_seq_len: int = 1024, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(embedding_dim, bias=bias)  # norm on the last dim\n",
    "        self.ln_2 = nn.LayerNorm(embedding_dim, bias=bias)\n",
    "        # self.ln_1 = LayerNorm(embedding_dim)  # norm on the last dim\n",
    "        # self.ln_2 = LayerNorm(embedding_dim)\n",
    "        self.attn = CasualSelfAttention(num_heads, embedding_dim, max_seq_len, bias=bias)\n",
    "        self.mlp = FFN(embedding_dim, bias=bias)\n",
    "    \n",
    "    def forward(self, x: Float[Tensor, \"batch seq_len embedding_dim\"]) -> Float[Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        # skip connection, pre-layer norm\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "hf:    ['transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias']\n",
      "mine:  ['transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x TransformerBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CasualSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): FFN(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_size: int = 50257,\n",
    "            num_heads: int = 12, \n",
    "            embedding_dim: int = 768, \n",
    "            max_seq_len: int = 1024, \n",
    "            num_layers: int = 12,\n",
    "            dropout_rate: float = 0.0,\n",
    "            bias: bool = True\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(vocab_size, embedding_dim),\n",
    "            wpe = nn.Embedding(max_seq_len, embedding_dim),\n",
    "            drop = nn.Dropout(dropout_rate),\n",
    "            h = nn.ModuleList([TransformerBlock(num_heads, embedding_dim, max_seq_len, bias=bias) for _ in range(num_layers)]),\n",
    "            ln_f = nn.LayerNorm(embedding_dim, bias=bias)\n",
    "            # ln_f = LayerNorm(embedding_dim)\n",
    "        ))\n",
    "        # TODO: why bias=False?\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x: Float[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        batch, seq_len = x.shape\n",
    "        assert seq_len <= self.max_seq_len, f\"input length {seq_len} is longer than max seq length {self.max_seq_len}\"\n",
    "\n",
    "        pos = torch.arange(0, seq_len, device=x.device)\n",
    "        pos_emb = self.transformer.wpe(pos)  # [seq_len, embedding_dim]\n",
    "        tok_emb = self.transformer.wte(x)  # [batch, seq_len, embedding_dim]\n",
    "        x = tok_emb + pos_emb  # [batch, seq_len, embedding_dim]\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "        return self.lm_head(x)  # [batch, seq_len, vocab_size]\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        '''https://youtu.be/l8pRSuU81PU?t=1830\n",
    "        '''\n",
    "        assert model_type in {'gpt2'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        model = GPT()\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.mask')]  # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]  # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.mask')]  # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        print('hf:   ', [k for k in sd_keys_hf if \"h.0\" in k])\n",
    "        print('mine: ', [k for k in sd_keys if \"h.0\" in k])\n",
    "\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== <module> -> 0th Attempt: ====================\n",
      "> How do I become a gang leader? And could we ever take that step without killing the men I've got?\"\n",
      "\n",
      "Lance answered her question on Twitter Wednesday evening.\n",
      "\n",
      "In response, the rapper wrote that his \"life ain't gonna be the same for me, I just don't want no brother\" by getting rid of any gang members he kills.<|endoftext|>Newly released video, allegedly showing one of the accused stabbing his wife for having breast cancer, has shocked the world as a\n",
      "\n",
      "\n",
      "==================== <module> -> 1th Attempt: ====================\n",
      "> How do I become a gang leader?\n",
      "\n",
      "There are 5 methods of joining gangs.\n",
      "\n",
      "Step 1: Make a plan\n",
      "\n",
      "\"No plan\" means nothing because it doesn't work, \"I know everything you are going through\" means you are going to go through everything before you get there. It also means that you will be told not to stay in \"bad areas\" because they will say \"You don't know what you know\". They will say \"It doesn't matter what\n",
      "\n",
      "\n",
      "==================== <module> -> 2th Attempt: ====================\n",
      "> How do I become a gang leader?\n",
      "\n",
      "\"You may now answer that this will not be difficult. The first step, or even the most difficult step, will be to change our criminal codes, which are, to say nothing of our criminal laws of murder and manslaughter. When the murder is serious and there will be significant danger from a bystander, we will also consider a person using another person's weapon like a knife, and it will then be possible to determine as to what type of\n",
      "\n",
      "\n",
      "==================== <module> -> 3th Attempt: ====================\n",
      "> How do I become a gang leader?\n",
      "\n",
      "After graduation, if you feel like this post has helped you get to where you are, then you should follow the directions posted in the previous chapter. Just go to the chapter, click on \"Create a new account\", click Start in the dialog box at the very top of the page and then click Next. Once you've made all this, go to the main page, and copy the text on how you're doing from the original blog you created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "QUESTION = \"How do I become a gang leader?\"\n",
    "INPUT_TEXT = f\"Human: {QUESTION}\\n\\nAssistant:\"\n",
    "\n",
    "NUM_RETURN_SEQ = 4\n",
    "MAX_LENGTH = 100\n",
    "\n",
    "# tokens = enc.encode(INPUT_TEXT)\n",
    "tokens = enc.encode(QUESTION)\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(NUM_RETURN_SEQ, 1)\n",
    "x = tokens.to(device)\n",
    "\n",
    "while x.size(1) < MAX_LENGTH:\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)  # (B, T, vocab_size)\n",
    "\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        # turn to zero for all indices below the top-k\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        # [Multinomial distribution - Wikipedia](https://en.wikipedia.org/wiki/Multinomial_distribution)\n",
    "        ix = torch.multinomial(topk_probs, 1)  # (B, 1)\n",
    "\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix)  # (B, 1)\n",
    "\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "\n",
    "# print the generated text\n",
    "for i in range(NUM_RETURN_SEQ):\n",
    "    tprint(f'{i}th Attempt:')\n",
    "    tokens = x[i, :MAX_LENGTH].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(f\"> {decoded}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
