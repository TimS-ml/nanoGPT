{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on v1's output and char_v6 notebook, implement a GPT2\n",
        "\n",
        "```\n",
        "transformer.wte.weight torch.Size([50257, 768])\n",
        "transformer.wpe.weight torch.Size([1024, 768])\n",
        "transformer.h.0.ln_1.weight torch.Size([768])\n",
        "transformer.h.0.ln_1.bias torch.Size([768])\n",
        "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
        "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
        "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
        "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
        "transformer.h.0.ln_2.weight torch.Size([768])\n",
        "transformer.h.0.ln_2.bias torch.Size([768])\n",
        "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
        "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
        "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
        "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
        "transformer.h.1.ln_1.weight torch.Size([768])\n",
        "transformer.h.1.ln_1.bias torch.Size([768])\n",
        "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
        "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
        "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
        "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
        "transformer.h.1.ln_2.weight torch.Size([768])\n",
        "transformer.h.1.ln_2.bias torch.Size([768])\n",
        "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
        "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
        "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
        "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
        "transformer.h.2.ln_1.weight torch.Size([768])\n",
        "transformer.h.2.ln_1.bias torch.Size([768])\n",
        "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
        "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
        "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
        "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
        "transformer.h.2.ln_2.weight torch.Size([768])\n",
        "transformer.h.2.ln_2.bias torch.Size([768])\n",
        "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
        "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
        "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
        "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
        "transformer.h.3.ln_1.weight torch.Size([768])\n",
        "transformer.h.3.ln_1.bias torch.Size([768])\n",
        "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
        "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
        "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
        "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
        "transformer.h.3.ln_2.weight torch.Size([768])\n",
        "transformer.h.3.ln_2.bias torch.Size([768])\n",
        "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
        "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
        "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
        "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
        "transformer.h.4.ln_1.weight torch.Size([768])\n",
        "transformer.h.4.ln_1.bias torch.Size([768])\n",
        "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
        "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
        "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
        "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
        "transformer.h.4.ln_2.weight torch.Size([768])\n",
        "transformer.h.4.ln_2.bias torch.Size([768])\n",
        "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
        "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
        "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
        "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
        "transformer.h.5.ln_1.weight torch.Size([768])\n",
        "transformer.h.5.ln_1.bias torch.Size([768])\n",
        "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
        "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
        "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
        "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
        "transformer.h.5.ln_2.weight torch.Size([768])\n",
        "transformer.h.5.ln_2.bias torch.Size([768])\n",
        "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
        "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
        "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
        "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
        "transformer.ln_f.weight torch.Size([768])\n",
        "transformer.ln_f.bias torch.Size([768])\n",
        "lm_head.weight torch.Size([50257, 768])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MF5SMVST_0Qj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "# from transformers import GPT2LMHeadModel\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "from utils import *\n",
        "from data_structure import add_to_class\n",
        "\n",
        "init_graph()\n",
        "device = get_device()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 256\n",
        "    vocab_size: int = 65\n",
        "    n_layer: int = 6\n",
        "    n_head: int = 6\n",
        "    n_embd: int = 384\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict()\n",
        "        \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
