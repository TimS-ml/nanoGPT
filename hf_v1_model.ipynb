{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "from utils import *; from boring_utils.utils import *\n",
    "\n",
    "init_graph()\n",
    "set_seed(42)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we gonna do some visualization, let's put it on cpu\n",
    "model_type = 'distilgpt2'\n",
    "# model_type = 'gpt2'\n",
    "\n",
    "model = AutoModel.from_pretrained(model_type, output_attentions=True)\n",
    "model_clm = AutoModelForCausalLM.from_pretrained(model_type)\n",
    "tokenizer_hf = AutoTokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HF Export Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor([[ 464, 3797, 3332,  319,  262, 2603]])\n",
    "inputs = tokenizer_hf.encode(\"The cat sat on the mat\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`GPT2SdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "outputs = model(\n",
    "        inputs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/Users/tim/miniforge3/envs/torch/lib/python3.11/site-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "outputs_dict = model_clm.generate(\n",
    "        inputs, \n",
    "        output_attentions=True,  # we need to add this line to get the attention\n",
    "        return_dict_in_generate=True,\n",
    "        use_cache = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare AutoModel(inputs) vs AutoModelForCausalLM.generate(inputs) and their attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions,\n",
       " transformers.generation.utils.GenerateDecoderOnlyOutput)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs), type(outputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m<module> -> outputs.keys():\u001b[0m\n",
      "odict_keys(['last_hidden_state', 'past_key_values', 'attentions'])\n",
      "\u001b[93m<module> -> outputs_dict.keys():\u001b[0m\n",
      "odict_keys(['sequences', 'attentions', 'past_key_values'])\n"
     ]
    }
   ],
   "source": [
    "cprint(outputs.keys(), outputs_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Since there is no \"generation head\" for AutoModel, so the \"attentions\" here is more like the embedding purpuse, i.e. the input embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs[-1] == outputs['attentions']  # True\n",
    "# outputs['attentions'][0] == outputs_dict['attentions'][0][0]  # True\n",
    "# outputs['attentions'][0][0] == outputs_dict['attentions'][0][0].squeeze(0)  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprint(len(inputs))\n",
    "cprint(outputs_dict[\"attentions\"][0][0].shape, outputs_dict[\"attentions\"][0][-1].shape)\n",
    "cprint(outputs_dict[\"attentions\"][-1][0].shape, outputs_dict[\"attentions\"][-1][-1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bertviz Head View\n",
    "\n",
    "```\n",
    "pip install bertviz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bertviz import head_view\n",
    "\n",
    "# attention = outputs[-1]  # Output includes attention weights when output_attentions=True\n",
    "# tokens = tokenizer_hf.convert_ids_to_tokens(inputs[0])\n",
    "# head_view(attention, tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
